# DIP Defaults for minimal VM Setup
{% set es_mdi = true %}
{% set es_master_java_memory = 1 %}
{% set es_master_memory = 1 %}
{% set es_master_num_procs = "1000m" %}
{% set es_master_count = 3 %}
{% set es_data_java_memory = 1 %}
{% set es_data_memory = 1 %}
{% set es_data_num_procs = "1000m" %}
{% set es_data_count = 2 %}
{% set es_coordinating_java_memory = 1 %}
{% set es_coordinating_memory = 1 %}
{% set es_coordinating_num_procs = "1000m" %}
{% set es_coordinating_count = groups['servers']|length %}
{% set kibana_memory = 1 %}
{% set kibana_cpu = "1000m" %}
{% set kibana_count = 1 %}

# R440 Kit
{% if kit_size == 2 %}
  {% set es_mdi = false %}
  {% set es_master_java_memory = 15 %}
  {% set es_master_memory = 15 %}
  {% set es_master_num_procs = "3000m" %}
  {% set es_master_count = 3 %}
  {% set es_data_java_memory = 30 %}
  {% set es_data_memory = 64 %}
  {% set es_data_num_procs = "6000m" %}
  {% set es_data_count = 6 %}
  {% set es_coordinating_java_memory = 30 %}
  {% set es_coordinating_memory = 45 %}
  {% set es_coordinating_num_procs = "3000m" %}
  {% set es_coordinating_count = groups['servers']|length %}
  {% set kibana_memory = 6 %}
  {% set kibana_cpu = "1000m" %}
  {% set kibana_count = 2 %}
# Small DL160 Kit
{% elif kit_size == 1 %}
  {% set es_mdi = true %}
  {% set es_master_java_memory = 20 %}
  {% set es_master_memory = 30 %}
  {% set es_master_num_procs = "8000m" %}
  {% set es_master_count = 3 %}
  {% set es_coordinating_java_memory = 30 %}
  {% set es_coordinating_memory = 45 %}
  {% set es_coordinating_num_procs = "3000m" %}
  {% set es_coordinating_count = groups['servers']|length %}
  {% set kibana_memory = 1 %}
  {% set kibana_cpu = "1000m" %}
  {% set kibana_count = 1 %}
# Tiny DL160 Kit
{% elif kit_size == 0 %}
  {% set es_mdi = true %}
  {% set es_master_java_memory = 20 %}
  {% set es_master_memory = 20 %}
  {% set es_master_num_procs = "8000m" %}
  {% set es_master_count = 3 %}
  {% set es_coordinating_java_memory = 8 %}
  {% set es_coordinating_memory = 9 %}
  {% set es_coordinating_num_procs = "4000m" %}
  {% set es_coordinating_count = groups['servers']|length %}
  {% set kibana_memory = 1 %}
  {% set kibana_cpu = "1000m" %}
  {% set kibana_count = 1 %}
{% endif %}

---
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: tfplenum
spec:
  version: {{elastic_7_version}}
  image: elasticsearch/elasticsearch:{{elastic_7_version}}
  http:
    tls:
      certificate:
        secretName: elasticsearch-certificate
  nodeSets:
  - name: master
    count: {{es_master_count}}
    config:
{% if es_mdi %}
      node.master: true
      node.data: true
      node.ingest: true
{% else %}
      node.master: true
      node.data: false
      node.ingest: false
{% endif %}
      node.max_local_storage_nodes: "25"
      xpack.monitoring.collection.enabled: "true"
      node.attr.node_name: ${NODE_NAME}
      cluster.routing.allocation.awareness.attributes: node_name
      script.cache.max_size: "500"
      script.max_compilations_rate: "256/1m"
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          component: elasticsearch
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        - name: install-plugins
          command: ['sh', '-c', 'bin/elasticsearch-plugin install --batch {{ ' '.join(elasticsearch_plugins) }}']
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: role
                  operator: In
                  values:
                  - server
{% if es_mdi == false %}
        volumes:
        - name: elasticsearch-data
          emptyDir: {}
{% endif %}
        containers:
        - name: elasticsearch
          # specify resource limits and requests
          resources:
            requests:
              memory: {{es_master_memory}}Gi
              cpu: {{es_master_num_procs}}
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms{{es_master_java_memory}}g -Xmx{{es_master_java_memory}}g"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
{% if es_mdi %}
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: elastic-disks
        resources:
          requests:
            storage: 1Gi
{% endif %}
{% if es_mdi == false %}
  - name: data
    count: {{es_data_count}}
    config:
      node.master: false
      node.data: true
      node.ingest: true
      node.max_local_storage_nodes: "25"
      xpack.monitoring.collection.enabled: "true"
      node.attr.node_name: ${NODE_NAME}
      cluster.routing.allocation.awareness.attributes: node_name
      script.cache.max_size: "500"
      script.max_compilations_rate: "256/1m"
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          component: elasticsearch
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: role
                  operator: In
                  values:
                  - server
        containers:
        - name: elasticsearch
          # specify resource limits and requests
          resources:
            requests:
              memory: {{es_data_memory}}Gi
              cpu: {{es_data_num_procs}}
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms{{es_data_java_memory}}g -Xmx{{es_data_java_memory}}g"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: elastic-disks
        resources:
          requests:
            storage: 1Gi
{% endif %}
  - name: coordinating
    count: {{es_coordinating_count}}
    config:
      node.master: false
      node.data: false
      node.ingest: true
      node.max_local_storage_nodes: "25"
      xpack.monitoring.collection.enabled: "true"
      node.attr.node_name: ${NODE_NAME}
      cluster.routing.allocation.awareness.attributes: node_name
      script.cache.max_size: "500"
      script.max_compilations_rate: "256/1m"
    podTemplate:
      metadata:
        labels:
          # additional labels for pods
          component: elasticsearch
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: role
                  operator: In
                  values:
                  - server
        volumes:
        - name: elasticsearch-data
          emptyDir: {}
        containers:
        - name: elasticsearch
          # specify resource limits and requests
          resources:
            requests:
              memory: {{es_coordinating_memory}}Gi
              cpu: {{es_coordinating_num_procs}}
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms{{ es_coordinating_java_memory }}g -Xmx{{ es_coordinating_java_memory }}g"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  labels:
    component: elasticsearch
spec:
  selector:
    common.k8s.elastic.co/type: elasticsearch
    elasticsearch.k8s.elastic.co/cluster-name: tfplenum
    elasticsearch.k8s.elastic.co/statefulset-name: tfplenum-es-coordinating
  ports:
  - name: http
    port: {{elastic_port}}
    targetPort: http
  type: LoadBalancer

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-headless
  labels:
    component: elasticsearch-headless
spec:
  selector:
    common.k8s.elastic.co/type: elasticsearch
    elasticsearch.k8s.elastic.co/cluster-name: tfplenum
    elasticsearch.k8s.elastic.co/statefulset-name: tfplenum-es-coordinating
  ports:
  - name: http
    port: {{elastic_port}}
    targetPort: http
  clusterIP: None

...
