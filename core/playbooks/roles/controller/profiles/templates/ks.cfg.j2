# System authorization information
auth --enableshadow --passalgo=sha512
url --url http://{{ server_ip }}/offlinerepo/tfplenum
repo --name=tfplenum-BaseOS-offline-repo --baseurl=http://{{ server_ip }}/offlinerepo/tfplenum/BaseOS
repo --name=tfplenum-AppStream-offline-repo --baseurl=http://{{ server_ip }}/offlinerepo/tfplenum/AppStream
repo --name=tfplenum-Extras-offline-repo --baseurl=http://{{ server_ip }}/offlinerepo/tfplenum/Extras
text
# Run the Setup Agent on first boot
firstboot --disable
# Keyboard layouts
keyboard --vckeymap=us --xlayouts='us'
# System language
lang en_US.UTF-8
# Disable Firewalld
firewall --enabled
eula --agreed
#Reboot when its all over
reboot
# Network information
{% if node_index is defined %}
network --device={{ hostvars[node_index].mac | lower }} --bootproto=static --ip={{ hostvars[node_index].management_ipv4 }} --netmask={{ netmask }} --gateway={{ gateway }} --nameserver={{ dns }} --noipv6 --activate
network --hostname={{ node_index }}
{% else %}
network  --bootproto=dhcp --device=eth0 --noipv6 --activate
network  --hostname=localhost.localdomain
{% endif %}
# Root password
rootpw --iscrypted {{ rootpw.stdout }}
# System services
services --disabled="chronyd"
# System timezone
timezone {{ timezone }} --isUtc --ntpservers={{ ansible_controller_hostname }}

%pre
{% if node_index is defined and hostvars[node_index].deployment_type != "Virtual" %}
curl -XPOST -k -H 'Authorization: Bearer {{ node_state_api_key }}' -H 'Content-Type: application/json' https://{{ server_ip }}/api/kit/node/{{ node_index }}/status -d '{ "name": "provision", "inprogress": true, "error": false, "message": "" }'
{% endif %}

default_os_schema() {
  echo "volgroup vg_root pv.01" >> /tmp/partition_layout
  echo "logvol / --fstype=\"xfs\" --name=root --vgname=vg_root --grow --percent=20" >> /tmp/partition_layout
  echo "logvol /tmp --fstype=\"xfs\" --name=tmp --vgname=vg_root --grow --percent=5" >> /tmp/partition_layout
  echo "logvol /home --fstype=\"xfs\" --name=home --vgname=vg_root --grow --percent=10" >> /tmp/partition_layout
  echo "logvol /var --fstype=\"xfs\" --name=var --vgname=vg_root --grow --percent=35" >> /tmp/partition_layout
  echo "logvol /var/log --fstype=\"xfs\" --name=log --vgname=vg_root --grow --percent=20" >> /tmp/partition_layout
  echo "logvol /var/log/audit --fstype=\"xfs\" --name=audit --vgname=vg_root --grow --percent=3" >> /tmp/partition_layout
}

default_data_schema() {
  echo "volgroup vg_data pv.02" >> /tmp/partition_layout
  echo "logvol /data --fstype=\"xfs\" --size=1 --name=data --vgname=vg_data --grow" >> /tmp/partition_layout
}

all_software_raid() {
  local os_parition_size_MiB="$1"
  local is_uefi="$2"
  local raid_level="$3"
  shift; shift; shift
  local drive_list=("$@")

  drive_list=($(echo "${drive_list[@]}" | tr ' ' '\n' | sort))
  local drive_list_comma_sep=$(printf '%s,' "${drive_list[@]}")

  local boot_partitions=()
  for drive in "${drive_list[@]}"; do
    boot_partitions+=("raid.0_$drive")
  done

  local os_partitions=()
  for drive in "${drive_list[@]}"; do
    os_partitions+=("raid.1_$drive")
  done

  local data_partitions=()
  for drive in "${drive_list[@]}"; do
    data_partitions+=("raid.2_$drive")
  done

  echo "zerombr" > /tmp/partition_layout
  echo "bootloader --append=\"rhgb quiet crashkernel=auto\" --location=mbr --driveorder=${drive_list_comma_sep::-1} --iscrypted --password=grub.pbkdf2.sha512.10000.23C682D3DE1136612AC0B3C369A9B975A5D46A002BC3F450AB728722925654073EE29078AE1D72449343DDA75D39987090B6D1738E71B9C331D8FF6AD35C75A0.436922F8BF0EC820FA8A788C5BF3D5FBF6D694142E7DFD8637CBD519D91B250CBF9464FF998010941E7996CE8744C86BBDA7DCB9435A534ADC45A8A530FAC697" >> /tmp/partition_layout
  echo "clearpart --all --disklabel gpt --initlabel" >> /tmp/partition_layout
  for drive in "${drive_list[@]}"; do
    dd if=/dev/zero of=/dev/$drive bs=512 count=100
    wipefs -f -a /dev/$drive
    echo "part raid.0_$drive --fstype=\"raid\" --size=512 --asprimary --ondrive=$drive" >> /tmp/partition_layout
    if ! $is_uefi; then
      echo "part biosboot --fstype=biosboot --size=1 --ondisk=$drive" >> /tmp/partition_layout
    fi
    echo "part raid.1_$drive --fstype=\"raid\" --size=$os_parition_size_MiB --asprimary --ondrive=$drive" >> /tmp/partition_layout
    echo "part raid.2_$drive --fstype=\"raid\" --size=1 --grow --ondrive=$drive" >> /tmp/partition_layout
  done

  if [ "$is_uefi" = true ]; then
    echo "part /boot/efi --fstype=efi --grow --maxsize=200 --size=20" >> /tmp/partition_layout
  fi
  echo "raid /boot --device=\"boot\" --fstype=\"xfs\" --label=\"rhel7-boot\" --level=$raid_level ${boot_partitions[@]}" >> /tmp/partition_layout
  echo "raid pv.01 --fstype=\"lvmpv\" --device=\"pv.01\" --label=\"lvmpv-pv01\" --level=$raid_level ${os_partitions[@]}" >> /tmp/partition_layout
  default_os_schema
  echo "raid pv.02 --fstype=\"lvmpv\" --device=\"pv.02\" --label=\"lvmpv-pv02\" --level=$raid_level ${data_partitions[@]}" >> /tmp/partition_layout
  default_data_schema
}

os_provision_on_one_device() {
  local drive="$1"
  dd if=/dev/zero of=/dev/$drive bs=512 count=100
  wipefs -f -a /dev/$drive
  echo "zerombr" > /tmp/partition_layout
  echo "bootloader --append=\"rhgb quiet crashkernel=auto\" --location=mbr --boot-drive=$drive --iscrypted --password=grub.pbkdf2.sha512.10000.23C682D3DE1136612AC0B3C369A9B975A5D46A002BC3F450AB728722925654073EE29078AE1D72449343DDA75D39987090B6D1738E71B9C331D8FF6AD35C75A0.436922F8BF0EC820FA8A788C5BF3D5FBF6D694142E7DFD8637CBD519D91B250CBF9464FF998010941E7996CE8744C86BBDA7DCB9435A534ADC45A8A530FAC697" >> /tmp/partition_layout
  echo "clearpart --all" >> /tmp/partition_layout
  if [ "$is_uefi" = true ]; then
    echo "part /boot/efi --fstype=efi --grow --maxsize=200 --size=20 --ondrive=$drive" >> /tmp/partition_layout
  fi
  echo "part /boot --fstype=\"xfs\" --size=500 --ondrive=$drive" >> /tmp/partition_layout
  echo "part pv.01 --fstype=\"lvmpv\" --size=1 --grow --ondisk=$drive" >> /tmp/partition_layout
  default_os_schema
}

check_unexpected_cases_and_send_notification() {
  local nvme_drive_count="$1"
  local sd_drive_count="$2"
  local hw_model=$(dmidecode -s system-product-name)
  if [[ $hw_model == "PowerEdge XR4510c" ||
        $hw_model == "USAF Interceptor Platform" ||
        $hw_model == "PowerEdge R6515" ||
        $hw_model == "PowerEdge R440" ]]; then
    echo "Failed kickstart see notifications window in the web GUI for details."
    curl -XPOST -k -H 'Authorization: Bearer {{ node_state_api_key }}' -H 'Content-Type: application/json' https://{{ server_ip }}/api/kit/kickstart_failure -d "{ \"hardware_model\": \"$hw_model\", \"nvme_drive_count\": $nvme_drive_count, \"sd_drive_count\": $sd_drive_count }"
    exit 1
  fi
}

os_parition_size_GB=100
sd_drives=($(lsblk -d -o NAME,TYPE | awk '/^sd/ && $2 == "disk" {print $1}'))
sd_drive_list_comma_sep=$(printf '%s,' "${sd_drives[@]}")
{% raw %}sd_drive_count=${#sd_drives[@]}
{% endraw %}

if [ "$sd_drive_count" -eq 0 ]; then
  #Dividing by zero makes the whole script fail
  os_parition_size_MiB=$(($os_parition_size_GB * 1024 / 1))
else
  os_parition_size_MiB=$(($os_parition_size_GB * 1024 / $sd_drive_count))
fi

if [ -d "/sys/firmware/efi/" ]; then
  is_uefi=true
else
  is_uefi=false
fi

{% if hostvars[node_index].raid0_override %}
is_raid_0_override=true
{% else %}
is_raid_0_override=false
{% endif %}

nvme_drives=($(lsblk -d -o NAME,TYPE | awk '/^nvme/ && $2 == "disk" {print $1}'))
nvme_drives=($(echo "${nvme_drives[@]}" | tr ' ' '\n' | sort))
nvme_drive_list_comma_sep=$(printf '%s,' "${nvme_drives[@]}")
{% raw %}nvme_drive_count=${#nvme_drives[@]}
{% endraw %}

if [ "$nvme_drive_count" -eq 0 ]; then
  #Dividing by zero makes the whole script fail.
  nvme_os_parition_size_MiB=$(($os_parition_size_GB * 1024 / 1))
else
  nvme_os_parition_size_MiB=$(($os_parition_size_GB * 1024 / $nvme_drive_count))
fi

# os_raid_level is basically the same as sd_raid_level with the only difference is that you cannot raid 0 override it
# OS drive does not require the performance of the data drive so we never allow raid 0 because of the redudancy problem
if [ "$sd_drive_count" -ge 3 ]; then
  os_raid_level=5
elif [ "$sd_drive_count" -eq 2 ]; then
  os_raid_level=1
else
  os_raid_level=0
fi

if [ "$is_raid_0_override" = true ]; then
  sd_raid_level=0
elif [ "$sd_drive_count" -ge 3 ]; then
  sd_raid_level=5
elif [ "$sd_drive_count" -eq 2 ]; then
  sd_raid_level=1
else
  sd_raid_level=0
fi

if [ "$is_raid_0_override" = true ]; then
  raid_level=0
elif [ "$nvme_drive_count" -ge 3 ]; then
  raid_level=5
elif [ "$nvme_drive_count" -eq 2 ]; then
  raid_level=1
else
  raid_level=0
fi

sd_boot_partitions=()
for drive in "${sd_drives[@]}"; do
  sd_boot_partitions+=("raid.0_$drive")
done

sd_os_partitions=()
for drive in "${sd_drives[@]}"; do
  sd_os_partitions+=("raid.1_$drive")
done

data_partitions=()
for drive in "${nvme_drives[@]}"; do
  data_partitions+=("raid.2_$drive")
done

# If the sd drive count has no values and all we have NVME drives
# We so a pure software raid in this case.
if [ $nvme_drive_count -gt 0 ]; then

  if [[ $(dmidecode -s system-product-name) == "PowerEdge XR4510c" ]]; then
    # Edge case here because we expect 3 NVME drives one is a 450GB while the other two are 7.3 TB
    nvme_drives=($(lsblk -b -d -o NAME,SIZE -I 259,258 | sort -k 2 -n | awk 'NR>1 {print $1}'))
    os_provision_on_one_device "${nvme_drives[0]}"
    nvme_drives=("${nvme_drives[@]:1}") # remove first element
    {% raw %}nvme_drive_count=${#nvme_drives[@]}
    {% endraw %}

    data_partitions=()
    for drive in "${nvme_drives[@]}"; do
      data_partitions+=("raid.2_$drive")
    done

    if [ "$is_raid_0_override" = true ]; then
      raid_level=0
    elif [ "$nvme_drive_count" -ge 3 ]; then
      raid_level=5
    elif [ "$nvme_drive_count" -eq 2 ]; then
      raid_level=1
    else
      raid_level=0
    fi

    for drive in "${nvme_drives[@]}"; do
      dd if=/dev/zero of=/dev/$drive bs=512 count=100
      wipefs -f -a /dev/$drive
      echo "part raid.2_$drive --fstype=\"raid\" --size=1 --grow --ondrive=$drive" >> /tmp/partition_layout
    done
    echo "raid pv.02 --fstype=\"lvmpv\" --device=\"pv.02\" --label=\"lvmpv-pv02\" --level=$raid_level ${data_partitions[@]}" >> /tmp/partition_layout
    default_data_schema
  elif [ $sd_drive_count -eq 0 ]; then
    # This handles the case where no SD(x) drives are present and only NVME are available.
    # This block will create a software RAID and then parition the OS and data drives separatley.
    check_unexpected_cases_and_send_notification "$nvme_drive_count" "$sd_drive_count"
    all_software_raid "$nvme_os_parition_size_MiB" "$is_uefi" "$raid_level" "${nvme_drives[@]}"
  elif [ $sd_drive_count -eq 1 ]; then
    # This is the expected configuration for the Dell 6515s one hardware raided sda drive set in RAID 1 mode
    # and 4 nvme drives
    os_provision_on_one_device "${sd_drives[0]}"
    for drive in "${nvme_drives[@]}"; do
      dd if=/dev/zero of=/dev/$drive bs=512 count=100
      wipefs -f -a /dev/$drive
      echo "part raid.2_$drive --fstype=\"raid\" --size=1 --grow --ondrive=$drive" >> /tmp/partition_layout
    done
    echo "raid pv.02 --fstype=\"lvmpv\" --device=\"pv.02\" --label=\"lvmpv-pv02\" --level=$raid_level ${data_partitions[@]}" >> /tmp/partition_layout
    default_data_schema
  else
    # This block handles the case where they forgot to do a hardware raid of the sda and sdb drives with NVMe drives present.
    # This will do a raid 1 or raid 5 depending on number of sd(x) drives, then do a separate software raid of the NVMe drives.
    check_unexpected_cases_and_send_notification "$nvme_drive_count" "$sd_drive_count"
    echo "zerombr" > /tmp/partition_layout
    echo "bootloader --append=\"rhgb quiet crashkernel=auto\" --location=mbr --driveorder=${sd_drive_list_comma_sep::-1} --iscrypted --password=grub.pbkdf2.sha512.10000.23C682D3DE1136612AC0B3C369A9B975A5D46A002BC3F450AB728722925654073EE29078AE1D72449343DDA75D39987090B6D1738E71B9C331D8FF6AD35C75A0.436922F8BF0EC820FA8A788C5BF3D5FBF6D694142E7DFD8637CBD519D91B250CBF9464FF998010941E7996CE8744C86BBDA7DCB9435A534ADC45A8A530FAC697" >> /tmp/partition_layout
    echo "clearpart --all --disklabel gpt --initlabel" >> /tmp/partition_layout
    for drive in "${sd_drives[@]}"; do
      dd if=/dev/zero of=/dev/$drive bs=512 count=100
      wipefs -f -a /dev/$drive
      echo "part raid.0_$drive --fstype=\"raid\" --size=512 --asprimary --ondrive=$drive" >> /tmp/partition_layout
      if ! $is_uefi; then
        echo "part biosboot --fstype=biosboot --size=1 --ondisk=$drive" >> /tmp/partition_layout
      fi
      echo "part raid.1_$drive --fstype=\"raid\" --size=1 --grow --asprimary --ondrive=$drive" >> /tmp/partition_layout
    done

    if [ "$is_uefi" = true ]; then
      echo "part /boot/efi --fstype=efi --grow --maxsize=200 --size=20" >> /tmp/partition_layout
    fi

    echo "raid /boot --device=\"boot\" --fstype=\"xfs\" --label=\"rhel7-boot\" --level=$os_raid_level ${sd_boot_partitions[@]}" >> /tmp/partition_layout
    echo "raid pv.01 --fstype=\"lvmpv\" --device=\"pv.01\" --label=\"lvmpv-pv01\" --level=$os_raid_level ${sd_os_partitions[@]}" >> /tmp/partition_layout
    default_os_schema

    for drive in "${nvme_drives[@]}"; do
      dd if=/dev/zero of=/dev/$drive bs=512 count=100
      wipefs -f -a /dev/$drive
      echo "part raid.2_$drive --fstype=\"raid\" --size=1 --grow --ondrive=$drive" >> /tmp/partition_layout
    done
    echo "raid pv.02 --fstype=\"lvmpv\" --device=\"pv.02\" --label=\"lvmpv-pv02\" --level=$raid_level ${data_partitions[@]}" >> /tmp/partition_layout
    default_data_schema
  fi
else
  if [ "$sd_drive_count" -eq 1 ]; then
    check_unexpected_cases_and_send_notification "$nvme_drive_count" "$sd_drive_count"
    # This block handles all cases where NVME drives are not present.  If sd drive count is 1 we create two partitions one for OS drive and one for data.
    dd if=/dev/zero of=/dev/${sd_drives[0]} bs=512 count=100
    wipefs -f -a /dev/${sd_drives[0]}
    echo "zerombr" > /tmp/partition_layout
    echo "bootloader --append=\"rhgb quiet crashkernel=auto\" --location=mbr --boot-drive=${sd_drives[0]} --iscrypted --password=grub.pbkdf2.sha512.10000.23C682D3DE1136612AC0B3C369A9B975A5D46A002BC3F450AB728722925654073EE29078AE1D72449343DDA75D39987090B6D1738E71B9C331D8FF6AD35C75A0.436922F8BF0EC820FA8A788C5BF3D5FBF6D694142E7DFD8637CBD519D91B250CBF9464FF998010941E7996CE8744C86BBDA7DCB9435A534ADC45A8A530FAC697" >> /tmp/partition_layout
    echo "clearpart --all" >> /tmp/partition_layout
    if [ "$is_uefi" = true ]; then
      echo "part /boot/efi --fstype=efi --grow --maxsize=200 --size=20 --ondrive=${sd_drives[0]}" >> /tmp/partition_layout
    fi
    echo "part /boot --fstype=\"xfs\" --size=500 --ondrive=${sd_drives[0]}" >> /tmp/partition_layout
    echo "part pv.01 --fstype=\"lvmpv\" --size=$os_parition_size_MiB --asprimary --ondisk=${sd_drives[0]}" >> /tmp/partition_layout
    default_os_schema
    echo "part pv.02 --fstype=\"lvmpv\" --size=1 --grow --ondisk=${sd_drives[0]}" >> /tmp/partition_layout
    default_data_schema
  elif [ "$sd_drive_count" -eq 2 ]; then
    # This block handles all cases where NVME drives are not present.  If sd drive count is 2 we assume that hardware raiding has been done correctly.
    sd1_path=/dev/${sd_drives[0]}
    sd2_path=/dev/${sd_drives[1]}
    dd if=/dev/zero of=$sd1_path bs=512 count=100
    dd if=/dev/zero of=$sd2_path bs=512 count=100
    wipefs -f -a $sd1_path
    wipefs -f -a $sd2_path

    sd1_size=$(fdisk -l $sd1_path | grep "Disk $sd1_path" | awk '{print $5}')
    sd2_size=$(fdisk -l $sd2_path | grep "Disk $sd2_path" | awk '{print $5}')

    # Compare the sizes of the two devices
    if [ $sd1_size -gt $sd2_size ]; then
      smaller_device="${sd_drives[1]}"
      larger_device="${sd_drives[0]}"
    else
      smaller_device="${sd_drives[0]}"
      larger_device="${sd_drives[1]}"
    fi

    echo "zerombr" > /tmp/partition_layout
    echo "bootloader --append=\"rhgb quiet crashkernel=auto\" --location=mbr --boot-drive=$smaller_device --iscrypted --password=grub.pbkdf2.sha512.10000.23C682D3DE1136612AC0B3C369A9B975A5D46A002BC3F450AB728722925654073EE29078AE1D72449343DDA75D39987090B6D1738E71B9C331D8FF6AD35C75A0.436922F8BF0EC820FA8A788C5BF3D5FBF6D694142E7DFD8637CBD519D91B250CBF9464FF998010941E7996CE8744C86BBDA7DCB9435A534ADC45A8A530FAC697" >> /tmp/partition_layout
    echo "clearpart --all" >> /tmp/partition_layout
    if [ "$is_uefi" = true ]; then
      echo "part /boot/efi --fstype=efi --grow --maxsize=200 --size=20 --ondrive=$smaller_device" >> /tmp/partition_layout
    fi
    echo "part /boot --fstype=\"xfs\" --size=500 --ondrive=$smaller_device" >> /tmp/partition_layout
    echo "part pv.01 --fstype=\"lvmpv\" --size=1 --grow --ondisk=$smaller_device" >> /tmp/partition_layout
    default_os_schema
    echo "part pv.02 --fstype=\"lvmpv\" --size=1 --grow --ondisk=$larger_device" >> /tmp/partition_layout
    default_data_schema
  else
    # If the sd drive count is 1 or greater than 2 we assume a software raid needs to be done.
    # Software raid all the sd(x) drives create needed partitions
    check_unexpected_cases_and_send_notification "$nvme_drive_count" "$sd_drive_count"
    all_software_raid "$os_parition_size_MiB" "$is_uefi" "$sd_raid_level" "${sd_drives[@]}"
  fi
fi

%end

%include /tmp/partition_layout

%packages --ignoremissing
@^minimal-environment
chrony
network-scripts
{% if node_index is defined and hostvars[node_index].deployment_type == "Virtual" %}
open-vm-tools
perl
{% endif %}
%end

%post --log=/root/ks-post.log
/usr/sbin/grubby --update-kernel=ALL --args=console=ttyS0

{% if node_index is defined %}
ip addr | grep -i broadcast | awk '{ print $2 }' > /tmp/interfaces
sed -i 's/:/\ /g' /tmp/interfaces
interfaces=`cat /tmp/interfaces`
for i in $interfaces;
do
mac=`ethtool -P $i | awk '{ print $3 }'`
if [[ $mac != '{{ hostvars[node_index].mac | lower }}' ]]; then
# This updates the non management interfaces to remove any DNS entries
sed -i '/^DNS/ d' /etc/sysconfig/network-scripts/ifcfg-$i
# This updates the non management interfaces to remove the NETBOOT line
sed -i '/^NETBOOT/ d' /etc/sysconfig/network-scripts/ifcfg-$i
# This updates the non management interfaces to manual
sed -i 's/^\(BOOTPROTO=\).*/\1none/g' /etc/sysconfig/network-scripts/ifcfg-$i
# This updates the non management interfaces to ignore ipv6 by default
sed -i 's/^\(IPV6INIT=\).*/\1no/g' /etc/sysconfig/network-scripts/ifcfg-$i
# Remove ONBOOT line from non management interface
sed -i '/^ONBOOT/ d' /etc/sysconfig/network-scripts/ifcfg-$i
# Append ONBOOT option based on link status of yes or no
echo 'ONBOOT=no' >> /etc/sysconfig/network-scripts/ifcfg-$i
sed -i '/^NETMASK/ d' /etc/sysconfig/network-scripts/ifcfg-$i
sed -i '/^IPADDR/ d' /etc/sysconfig/network-scripts/ifcfg-$i
fi
done
{% endif %}

# SELinux for data dir
chcon -R -t container_file_t /data

# Disable SSHD DNS lookups
echo "UseDNS no" >> /etc/ssh/sshd_config

# Add DNS search domain
{% if node_index is defined -%}
echo "DOMAIN={{ kit_domain }}" >> `grep -r '{{ hostvars[node_index].management_ipv4 }}' /etc/sysconfig/ -l`
{%- endif %}

systemctl enable chronyd
sed -i 's/^pool.*/server {{ ansible_controller_hostname }} iburst/g' /etc/chrony.conf

mkdir /root/.ssh
echo "{{ controller_public_key }}" > /root/.ssh/authorized_keys
chmod 0600 /root/.ssh/authorized_keys
restorecon -R /root/.ssh/

cat <<EOF > /etc/yum.repos.d/offline.repo
[tfplenum-BaseOS-offline-repo]
baseurl=http://{{ansible_controller_hostname}}/offlinerepo/tfplenum/BaseOS
enabled=1
gpgcheck=0
name=tfplenum-baseos-offline-repo

[tfplenum-AppStream-offline-repo]
baseurl=http://{{ansible_controller_hostname}}/offlinerepo/tfplenum/AppStream
enabled=1
gpgcheck=0
name=tfplenum-appstream-offline-repo

[tfplenum-Extras-offline-repo]
baseurl=http://{{ansible_controller_hostname}}/offlinerepo/tfplenum/Extras
enabled=1
gpgcheck=0
name=tfplenum-extras-offline-repo
EOF

%end
