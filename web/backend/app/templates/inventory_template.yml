---
# The inventory defines which hosts belong to which groups and what variables are applied to them. The playbooks
# themselves in our scheme do not define any variables. The names you see on the far left here correspond to the limit
# function you can run with the ansible-playbook command. For example ansible-playbook site.yml --limit sensor would
# look at this inventory file and see the hosts that are part of the sensor group. When site.yml imports the sensors.yml
# file, sensor.yml will only apply itself to the servers listed in the sensors group in this inventory file.

all:
  vars:

    #############################
    # General System Settings   #
    #############################

    # The IP address of the system DNS server. You may define this or it will default
    # to using the master server's management IP. We suggest you leave it to default
    # unless you have a specific reason to use a different DNS server. Keep in mind
    # you will need to manually provide all required DNS entries on your separate
    # DNS Server or the kit will break.
    dns_ip: {{ template_ctx.dns_ip }}


    #############################
    # CPU Allocations           #
    #############################
    logstash_cpu_limit: {{ template_ctx.server_cal.logstash_cpu_limit | int }}m
    logstash_cpu_request: {{ template_ctx.server_cal.logstash_cpu_request | int }}m

    ################################
    # Full packet capture settings #
    ################################

    # Which application to use for FPC. Valid settings are 'moloch', 'suricata',
    # or 'none'.
    fpc_app: suricata

    # We recommend you dedicate a separate disk to PCAP. The disk you will use
    # for PCAP is defined on the sensor hosts in pcap_disk. If you define this
    # value, that disk will be formatted and wiped and then /pcap used as a
    # mount point to it.
    fpc_path: /pcap

    # This is the amount of free space (as a percentage) to leave available on
    # the PCAP disk.
    fpc_free: 25

    # Max filesize for a single PCAP in GB
    fpc_max_filesize: 1

    #############################
    # Moloch Settings           #
    #############################

    # Explanations taken from: https://github.com/aol/moloch/wiki/Settings

    # The bpf filter used to reduce traffic. Used both on live and file traffic.
    moloch_bpf: ""

    # Semicolon ';' separated list of bpf filters which when matched for a
    # session causes the remaining pcap from being saved for the session. It is
    # possible to specify the number of packets to save per filter by ending
    # with a :num. For example dontSaveBPFs = port 22:5 will only save 5 packets
    # for port 22 sessions. Currently only the initial packet is matched against
    # the bpfs.
    moloch_dontSaveBPFs: ""

    # The block size in bytes used for reads from each interface. There are 120 blocks per interface.
    moloch_packet_v3_block_size: 8388608

    # The number of threads used to read packets from each interface. These threads take the packets
    # from the AF packet interface and place them into the packet queues.
    moloch_packet_v3_num_threads: 8

    # Advanced Moloch settings, do not touch unless you know what you're doing
    moloch_spiDataMaxIndices: "5"
    moloch_pcapWriteMethod: "thread-direct"
    moloch_pcapWriteSize: "2560000"
    moloch_dbBulkSize: "400000"
    moloch_maxESConns: "30"
    moloch_maxESRequests: "500"
    moloch_packetsPerPoll: "500000"
    moloch_magicMode: "basic"
    moloch_maxPacketsInQueue: "400000"

    #############################
    # Elasticsearch Settings    #
    #############################

    # The number of Elasticsearch data nodes you would like to run. These are
    # different from elastic_masters in that they do not run the master role
    # Unless your kit will exceed 5 nodes, you should probably leave this at 0
    # unless you know what you are doing.

    elastic_mdi: false
    es_min_masters: 2

    # data node
    elastic_data_cpu_request: {{ template_ctx.server_cal.elastic_data_cpu_request }}m
    elastic_data_jvm_memory: {{ template_ctx.server_cal.elastic_data_jvm_memory }}
    elastic_data_memory: {{ template_ctx.server_cal.elastic_data_memory }}
    elastic_datas: {{ template_ctx.server_cal.elastic_data_pod_count }}
    separate_data: true

    # master node
    elastic_master_memory: {{ template_ctx.server_cal.elastic_master_memory }}
    elastic_master_jvm_memory: {{ template_ctx.server_cal.elastic_master_jvm_memory }}
    elastic_master_cpu_request: {{ template_ctx.server_cal.elastic_master_cpu_request }}m
    elastic_masters: {{ template_ctx.server_cal.elastic_master_pod_count }}
    separate_master: true

    # The percentage of maximum allocated space for Elasticsearch that can be filled
    # before Curator begins deleting indices. The oldest moloch indices that exceed
    # this threshold will be deleted.
    elastic_curator_threshold: {{ template_ctx.server_cal.elastic_curator_threshold }}

    # logstash_replicas is the number of logstash instances you would like to run
    logstash_replicas: {{ template_ctx.server_cal.logstash_replicas }}
    logstash_memory_request: {{ template_ctx.server_cal.logstash_memory_request }}
    logstash_memory_limit: {{ template_ctx.server_cal.logstash_memory_limit }}
    logstash_jvm_memory: {{ template_ctx.server_cal.logstash_jvm_memory }}
    logstash_pipeline_workers: {{ template_ctx.server_cal.logstash_pipeline_workers }}

    #############################
    # Bro/Suricata Settings     #
    #############################

    # This is used to define the home nets for bro/suricata
    home_net:
    {% if template_ctx.sensor_resources.home_nets | length > 0 %}
      {% for item in template_ctx.sensor_resources.home_nets %}
      - "{{ item.home_net }}"
      {% endfor %}
    {% endif %}

    # This is used to define the external nets for bro/suricata
    external_net:
    {% if template_ctx.sensor_resources.external_nets | length > 0 %}
      {% for item in template_ctx.sensor_resources.external_nets %}
      - "{{ item.external_net }}"
      {% endfor %}
    {% endif %}

    #############################
    # Kubernetes Settings       #
    #############################

    # kubernetes_services_cidr is the range of addresses kubernetes will use for external services
    # This includes cockpit, Moloch viewer, Kibana, elastichq, and
    # the kubernetes dashboard. This range must be at least a /28. Ex: "192.168.1.16/28"
    kubernetes_services_cidr: "{{ template_ctx.kubernetes_services_cidr }}"

    #############################
    # End Game Settings       #
    #############################

    endgame_host: "{{ template_ctx.endgame_iporhost }}"
    endgame_username: "{{ template_ctx.endgame_username }}"
    endgame_password: "{{ template_ctx.endgame_password }}"

    #############################
    # Software Locations        #
    #############################

    # WARNING!!!: As a rule of thumb, you should not change any of these unless
    # you have a full understanding of the consequences.

  children:

    # Any host in this group will be eligible for use in the elasticsearch cluster.
    # By default this is all hosts except remote-sensors. Keep in mind, just because
    # a node is eligible to run Elasticsearch, doesn't mean it will - only that it
    # can. Kubernetes will decide where to place instances based on resource consumption.
    elasticsearch:
      children:
        servers:

    # Any host in this group will be eligible to run Logstash. By default this is
    # any host that also has Elasticsearch. Keep in mind, just because a node is
    # eligible to run Logstash, doesn't mean it will - only that it can.
    logstash:
      children:
        elasticsearch:

    # Any host in this group will be eligible to run kibana. By default this is
    # any host that also has Elasticsearch. Keep in mind, just because a node is
    # eligible to run Kibana, doesn't mean it will - only that it can. Kubernetes
    # will decide where to place instances based on resource consumption.
    kibana:
      children:
        elasticsearch:

    # Any host in this group will run Bro. By default this is all sensors and remote
    # sensors
    bro: {% if template_ctx.has_bro %}
      hosts: {% for node in template_ctx.nodes %}{% if node.node_type == "Sensor" %}{% if 'bro' in node.sensor_apps %}
        {{ node.hostname }}: {% endif %}{% endif %}{% endfor %}
      {% endif %}

    # Any host in this group will run Moloch. By default this is all sensors and
    # remote sensors
    moloch: {% if template_ctx.has_moloch %}
      hosts: {% for node in template_ctx.nodes %}{% if node.node_type == "Sensor" %}{% if 'moloch' in node.sensor_apps %}
        {{ node.hostname }}: {% endif %}{% endif %}{% endfor %}
      {% endif %}

    # Any host in this group will run Suricata. By default this is all sensors
    # and remote sensors
    suricata: {% if template_ctx.has_suricata %}
      hosts: {% for node in template_ctx.nodes %}{% if node.node_type == "Sensor" %}{% if 'suricata' in node.sensor_apps %}
        {{ node.hostname }}: {% endif %}{% endif %}{% endfor %}
      {% endif %}

    nodes:
      children:

    #############################
    # Sensor Settings           #
    #############################

    # Here you will define any variables specific to each sensor host.

        sensors:
          {% if template_ctx.sensor_count > 0 %}
          hosts:
          {% endif %}
          {% for node in template_ctx.nodes %}
            {% if node.node_type == "Sensor" %}
            # This is the hostname of the sensor. Whatever you put here will be
            # used to overwrite the current hostname.
            {{ node.hostname }}:

              # Applications
              suricata: {{ node.has_suricata }}
              bro: {{ node.has_bro }}
              moloch: {{ node.has_moloch }}

              # This is the user you will use to SSH to each box for setup. This
              # should always be root.
              ansible_user: root

              # This is the Ansible connection type. This should always be SSH.
              ansible_connection: ssh

              # The amount of milli CPUs that will be reserved for the host Operating system.
              # This is required so a Kubernetes pod does not accidentally starve a node of all
              # its resources.
              sys_cpu_reserve: {{ node.reservations.sys_cpu_reserve }}

              # The amount of KiB that will be reserved for the host operating system.
              # This is required so a Kubernetes pod does not accidentally starve a node of all
              # its resources.
              sys_mem_reserve: {{ node.reservations.sys_mem_reserve }}

              # The amount of milli CPUs that will be reserved for the kubernetes services.
              # This is required so a Kubernetes pod does not accidentally starve a node of all
              # its resources.
              kube_cpu_reserve: {{ node.reservations.kube_cpu_reserve }}

              # The amount of KiB that will be reserved for the kubernetes services.
              # This is required so a Kubernetes pod does not accidentally starve a node of all
              # its resources.
              kube_mem_reserve: {{ node.reservations.kube_mem_reserve }}

              # This is the management IP address of the node. Ex: 192.168.1.198
              # This is the address Ansible will use to communicate with.
              management_ipv4: "{{ node.management_ip_address }}"

              bro_cpu_request: {{ node.cal.bro_cpu_request | int }}m
              moloch_cpu_request: {{ node.cal.moloch_cpu_request | int }}m
              suricata_cpu_request: {{ node.cal.suricata_cpu_request | int }}m
              moloch_mem_limit: {{ node.cal.moloch_mem_limit | int }}Gi

              # This is the number of bro workers which will spawn on the sensor
              bro_workers: {{ node.cal.bro_workers }}

              # This is the number of threads which will be dedicated to Moloch
              moloch_threads: {{ node.cal.moloch_threads }}
              is_remote: {{ node.is_remote }}

              # This is the interface the sensor will use for monitoring on Moloch,
              # bro, and Suricata. Ex: ens4
              sensor_monitor_interface:
              {% for interface in node.monitor_interface %}
                - {{ interface }}{% endfor %}

              # The drive specified here will be dedicated to PCAP collection.
              pcap_disk: {% if node.pcap_drives is not none and node.pcap_drives|length > 0 %}{{ "/dev/" + node.pcap_drives[0] }}{% endif %}
            {% endif %}
          {% endfor %}

        # If you have a deployment where the majority of the kit is in one location,
        # but some sensors remotely deployed away from the central servers put them
        # in this group. This will remove any clustering dependencies among nodes.
        # For example: Moloch will not try to cluster with other moloch instances
        # nearby. If you have no remote sensors, leave the group defined, but
        #everything after the group name empty. Ex: you could have an inventory
        #with just "remote-sensors:", but no hosts defined.
        remote-sensors:

    #############################
    # Server Settings           #
    #############################

    # See the sensor section for explanation of values. Servers exist primarily
    # to run Elasticsearch and provide the horsepower to run the sensors. You can
    # see the group section for a list of things that run specifically on servers
    # only.

        servers:
          {% if template_ctx.server_count > 0 %}
          hosts:
          {% endif %}
            {% for node in template_ctx.nodes %}
            {% if node.node_type == "Server" %}
            {{ node.hostname }}:
              ansible_user: root
              ansible_connection: ssh
              sys_cpu_reserve: {{ node.reservations.sys_cpu_reserve }}
              sys_mem_reserve: {{ node.reservations.sys_mem_reserve }}
              kube_cpu_reserve: {{ node.reservations.kube_cpu_reserve }}
              kube_mem_reserve: {{ node.reservations.kube_mem_reserve }}
              management_ipv4: {{ node.management_ip_address }}
              is_master: {{ node.is_master_server }}
              es_data_drive_list:
              {% for drive in node.es_drives %}
                - {{ "/dev/" + drive }}
              {% endfor %}
            {% endif %}
            {% endfor %}

        master-server:

    nodes_to_remove:
      # Example: (you need to define your own hosts)
      #hosts:
      #  tfplenumsensor1.lan:
      #  tfplenumserver2.lan:
      #  tfplenumserver1.lan:

...
